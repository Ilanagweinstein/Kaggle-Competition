{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Cadent_Sentiment_Analysis_Bert.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tpnioR0TLaq4",
        "G19nqCPrLars",
        "b5qQXz7RLar-"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ilanagweinstein/Kaggle-Competition/blob/master/Cadent_Sentiment_Analysis_Bert_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwujU93ZUC2d",
        "colab_type": "text"
      },
      "source": [
        "## Kaggle Sentiment Analysis on Movie Reviews ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIVMY4-BdK-b",
        "colab_type": "text"
      },
      "source": [
        "**Loading transformers, data and initializing GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNdugWnaLapH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcnIFcqkLapN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ebz8IXtKLuU8",
        "colab_type": "code",
        "outputId": "0640843c-bb6a-4aad-9ded-c2d0541dc46a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "  print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "  raise SystemError('GPU device not found')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMwqMWdgbZ0W",
        "colab_type": "code",
        "outputId": "8dd2bcb2-abe5-42c4-a5fe-a300fe36ae56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch# If there's a GPU available...\n",
        "if torch.cuda.is_available():  # Tell PyTorch to use the GPU. \n",
        "  device = torch.device(\"cuda\") \n",
        "  print('There are %d GPU(s) available.' % torch.cuda.device_count()) \n",
        "  print('We will use the GPU:', torch.cuda.get_device_name(0))# If not...\n",
        "else:\n",
        "  print('No GPU available, using the CPU instead.')\n",
        "  device = torch.device(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoF8f73IeOdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLnGt1Ypfe30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install -q kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdAmiWHNhpt3",
        "colab_type": "code",
        "outputId": "a5eb9b17-6b22-4a94-c424-a265f73a0844",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c1d55b90-43cb-4e5b-a3b7-a25d63b2e14e\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-c1d55b90-43cb-4e5b-a3b7-a25d63b2e14e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ELHl9k5ldpn",
        "colab_type": "code",
        "outputId": "50e203fa-9955-4fc1-df24-38eec37fec8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-NWSV-4lgUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfMYpZGMlj61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zse9CUj_lp-K",
        "colab_type": "code",
        "outputId": "872073ef-28c2-431b-c3d6-4f7fe054857c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!kaggle competitions download -c sentiment-analysis-on-movie-reviews"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test.tsv.zip to /content\n",
            "  0% 0.00/494k [00:00<?, ?B/s]\n",
            "100% 494k/494k [00:00<00:00, 68.0MB/s]\n",
            "Downloading train.tsv.zip to /content\n",
            "  0% 0.00/1.28M [00:00<?, ?B/s]\n",
            "100% 1.28M/1.28M [00:00<00:00, 85.7MB/s]\n",
            "Downloading sampleSubmission.csv to /content\n",
            "  0% 0.00/583k [00:00<?, ?B/s]\n",
            "100% 583k/583k [00:00<00:00, 82.9MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U27nMTJl0X0",
        "colab_type": "code",
        "outputId": "365c8921-a254-481a-ba68-08ec8fe7f729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!unzip test.tsv.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  test.tsv.zip\n",
            "  inflating: test.tsv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifHuxmKOmAFc",
        "colab_type": "code",
        "outputId": "3ae01450-71bf-4210-bb3d-eb94d94bd9c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!unzip train.tsv.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  train.tsv.zip\n",
            "  inflating: train.tsv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykZkUaurLapR",
        "colab_type": "code",
        "outputId": "7a21ed5d-431b-4cd1-fef7-11ae0ac40a0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train = pd.DataFrame(pd.read_csv('/content/train.tsv', sep='\\t'))\n",
        "test = pd.DataFrame(pd.read_csv('/content/test.tsv', sep='\\t'))\n",
        "\n",
        "print('Number of training sentences: {:,}\\n'.format(train.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 156,060\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IqNPn8tLapX",
        "colab_type": "code",
        "outputId": "0c098d10-f7e1-4557-d4e3-2e0d37e97d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...  Sentiment\n",
              "0         1  ...          1\n",
              "1         2  ...          2\n",
              "2         3  ...          2\n",
              "3         4  ...          2\n",
              "4         5  ...          2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFfZyAYMLapa",
        "colab_type": "code",
        "outputId": "842eb0f8-8e56-43e3-d470-7e746dc53951",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>156061</td>\n",
              "      <td>8545</td>\n",
              "      <td>An intermittently pleasing but mostly routine ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>156062</td>\n",
              "      <td>8545</td>\n",
              "      <td>An intermittently pleasing but mostly routine ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>156063</td>\n",
              "      <td>8545</td>\n",
              "      <td>An</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>156064</td>\n",
              "      <td>8545</td>\n",
              "      <td>intermittently pleasing but mostly routine effort</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>156065</td>\n",
              "      <td>8545</td>\n",
              "      <td>intermittently pleasing but mostly routine</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  SentenceId                                             Phrase\n",
              "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
              "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
              "2    156063        8545                                                 An\n",
              "3    156064        8545  intermittently pleasing but mostly routine effort\n",
              "4    156065        8545         intermittently pleasing but mostly routine"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy8YUGnydeEJ",
        "colab_type": "text"
      },
      "source": [
        "**Analysis of data can be found in Cadent_Sentiment_Analysis_Basic**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4xnNHAwLaqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = train.drop(['Sentiment'], axis=1)\n",
        "y_train = train['Sentiment']\n",
        "\n",
        "X_test = test\n",
        "\n",
        "train_texts, train_labels = X_train['Phrase'], y_train\n",
        "val_texts = X_test['Phrase']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4dDsykFeLkg",
        "colab_type": "text"
      },
      "source": [
        "**Data Pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avew8uWPnv8C",
        "colab_type": "code",
        "outputId": "7410cf3a-be29-4428-def0-c1f65b9c1c20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#The following code was adapted from this Bert-fine-tuning example by Chris McCormick and Nick Ryan\n",
        "#Link: https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "#The same tutorial has been helpful for a current Fake Claim detection project I have been working on at NYU.\n",
        "\n",
        "#BERT tokenizer\n",
        "from transformers import BertTokenizer\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHtdYGTnoR3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_mask(texts):\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []# For every sentence...\n",
        "  attention_masks = []\n",
        "\n",
        "  for text in texts:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    \n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences. Max BERT can do is 512.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_dict['input_ids'])# Print sentence 0, now as a list of IDs.\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "  \n",
        "    # Convert the lists into tensors.\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  \n",
        "  #print out example\n",
        "  print('Original: ', texts[0])\n",
        "  print('Token IDs:', input_ids[0])\n",
        "  print('Attention Masks:', attention_masks[0])\n",
        "\n",
        "  return input_ids, attention_masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMk6Y1I1rMpA",
        "colab_type": "code",
        "outputId": "116d3684-279f-46a9-fd3e-d441a99e0959",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "#Train data: tokenize, pad, mask and convert to tensors\n",
        "train_inputs, train_masks = tokenize_mask(train_texts)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "\n",
        "#Val data: tokenize, pad, mask and convert to tensors\n",
        "val_inputs, val_masks = tokenize_mask(val_texts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .\n",
            "Token IDs: tensor([  101,  1037,  2186,  1997,  9686, 17695, 18673, 14313,  1996, 15262,\n",
            "         3351,  2008,  2054,  2003,  2204,  2005,  1996, 13020,  2003,  2036,\n",
            "         2204,  2005,  1996, 25957,  4063,  1010,  2070,  1997,  2029,  5681,\n",
            "         2572, 25581,  2021,  3904,  1997,  2029,  8310,  2000,  2172,  1997,\n",
            "         1037,  2466,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "Attention Masks: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Original:  An intermittently pleasing but mostly routine effort .\n",
            "Token IDs: tensor([  101,  2019, 23852,  2135, 24820,  2021,  3262,  9410,  3947,  1012,\n",
            "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "Attention Masks: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MYOWL6XegkW",
        "colab_type": "text"
      },
      "source": [
        "**Pre-trained Model and Training Set-up**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSfzPkYxsaiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler# The DataLoader needs to know our batch size for training\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)# Create the DataLoader for our validation set.\n",
        "\n",
        "# Create the DataLoader for validation set without labels.\n",
        "val_data = TensorDataset(val_inputs, val_masks)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAROsdsUtWcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        " \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        " num_labels = 5, # The number of output labels for multi-class classification. \n",
        " output_attentions = False, # Whether the model returns attentions weights.\n",
        " output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")# Tell pytorch to run this model on the GPU.\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMLOMxE3e4eo",
        "colab_type": "text"
      },
      "source": [
        "model.cuda() output removed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGtw1ayevvg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "optimizer = AdamW(model.parameters(),\n",
        " lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        " eps = 1e-8 # args.adam_epsilon - default is 1e-8.\n",
        " )\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        " num_warmup_steps = 0, # Default value in run_glue.py\n",
        " num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNYYeNFywR8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK1LwK5GfAEq",
        "colab_type": "text"
      },
      "source": [
        "#Training#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1l0NZC3yWSr",
        "colab_type": "code",
        "outputId": "bd8d80be-db64-4ee3-803d-3339ca7dfd0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "  \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Training Time': training_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  4,877.    Elapsed: 0:00:19.\n",
            "  Batch    80  of  4,877.    Elapsed: 0:00:38.\n",
            "  Batch   120  of  4,877.    Elapsed: 0:00:57.\n",
            "  Batch   160  of  4,877.    Elapsed: 0:01:17.\n",
            "  Batch   200  of  4,877.    Elapsed: 0:01:37.\n",
            "  Batch   240  of  4,877.    Elapsed: 0:01:57.\n",
            "  Batch   280  of  4,877.    Elapsed: 0:02:18.\n",
            "  Batch   320  of  4,877.    Elapsed: 0:02:38.\n",
            "  Batch   360  of  4,877.    Elapsed: 0:02:59.\n",
            "  Batch   400  of  4,877.    Elapsed: 0:03:21.\n",
            "  Batch   440  of  4,877.    Elapsed: 0:03:42.\n",
            "  Batch   480  of  4,877.    Elapsed: 0:04:03.\n",
            "  Batch   520  of  4,877.    Elapsed: 0:04:24.\n",
            "  Batch   560  of  4,877.    Elapsed: 0:04:45.\n",
            "  Batch   600  of  4,877.    Elapsed: 0:05:06.\n",
            "  Batch   640  of  4,877.    Elapsed: 0:05:27.\n",
            "  Batch   680  of  4,877.    Elapsed: 0:05:48.\n",
            "  Batch   720  of  4,877.    Elapsed: 0:06:09.\n",
            "  Batch   760  of  4,877.    Elapsed: 0:06:31.\n",
            "  Batch   800  of  4,877.    Elapsed: 0:06:52.\n",
            "  Batch   840  of  4,877.    Elapsed: 0:07:13.\n",
            "  Batch   880  of  4,877.    Elapsed: 0:07:34.\n",
            "  Batch   920  of  4,877.    Elapsed: 0:07:55.\n",
            "  Batch   960  of  4,877.    Elapsed: 0:08:16.\n",
            "  Batch 1,000  of  4,877.    Elapsed: 0:08:37.\n",
            "  Batch 1,040  of  4,877.    Elapsed: 0:08:58.\n",
            "  Batch 1,080  of  4,877.    Elapsed: 0:09:19.\n",
            "  Batch 1,120  of  4,877.    Elapsed: 0:09:40.\n",
            "  Batch 1,160  of  4,877.    Elapsed: 0:10:01.\n",
            "  Batch 1,200  of  4,877.    Elapsed: 0:10:22.\n",
            "  Batch 1,240  of  4,877.    Elapsed: 0:10:44.\n",
            "  Batch 1,280  of  4,877.    Elapsed: 0:11:05.\n",
            "  Batch 1,320  of  4,877.    Elapsed: 0:11:26.\n",
            "  Batch 1,360  of  4,877.    Elapsed: 0:11:47.\n",
            "  Batch 1,400  of  4,877.    Elapsed: 0:12:08.\n",
            "  Batch 1,440  of  4,877.    Elapsed: 0:12:29.\n",
            "  Batch 1,480  of  4,877.    Elapsed: 0:12:50.\n",
            "  Batch 1,520  of  4,877.    Elapsed: 0:13:11.\n",
            "  Batch 1,560  of  4,877.    Elapsed: 0:13:32.\n",
            "  Batch 1,600  of  4,877.    Elapsed: 0:13:53.\n",
            "  Batch 1,640  of  4,877.    Elapsed: 0:14:14.\n",
            "  Batch 1,680  of  4,877.    Elapsed: 0:14:35.\n",
            "  Batch 1,720  of  4,877.    Elapsed: 0:14:56.\n",
            "  Batch 1,760  of  4,877.    Elapsed: 0:15:18.\n",
            "  Batch 1,800  of  4,877.    Elapsed: 0:15:39.\n",
            "  Batch 1,840  of  4,877.    Elapsed: 0:16:00.\n",
            "  Batch 1,880  of  4,877.    Elapsed: 0:16:21.\n",
            "  Batch 1,920  of  4,877.    Elapsed: 0:16:42.\n",
            "  Batch 1,960  of  4,877.    Elapsed: 0:17:03.\n",
            "  Batch 2,000  of  4,877.    Elapsed: 0:17:24.\n",
            "  Batch 2,040  of  4,877.    Elapsed: 0:17:45.\n",
            "  Batch 2,080  of  4,877.    Elapsed: 0:18:06.\n",
            "  Batch 2,120  of  4,877.    Elapsed: 0:18:27.\n",
            "  Batch 2,160  of  4,877.    Elapsed: 0:18:48.\n",
            "  Batch 2,200  of  4,877.    Elapsed: 0:19:09.\n",
            "  Batch 2,240  of  4,877.    Elapsed: 0:19:30.\n",
            "  Batch 2,280  of  4,877.    Elapsed: 0:19:51.\n",
            "  Batch 2,320  of  4,877.    Elapsed: 0:20:13.\n",
            "  Batch 2,360  of  4,877.    Elapsed: 0:20:34.\n",
            "  Batch 2,400  of  4,877.    Elapsed: 0:20:55.\n",
            "  Batch 2,440  of  4,877.    Elapsed: 0:21:16.\n",
            "  Batch 2,480  of  4,877.    Elapsed: 0:21:37.\n",
            "  Batch 2,520  of  4,877.    Elapsed: 0:21:58.\n",
            "  Batch 2,560  of  4,877.    Elapsed: 0:22:19.\n",
            "  Batch 2,600  of  4,877.    Elapsed: 0:22:40.\n",
            "  Batch 2,640  of  4,877.    Elapsed: 0:23:01.\n",
            "  Batch 2,680  of  4,877.    Elapsed: 0:23:22.\n",
            "  Batch 2,720  of  4,877.    Elapsed: 0:23:44.\n",
            "  Batch 2,760  of  4,877.    Elapsed: 0:24:05.\n",
            "  Batch 2,800  of  4,877.    Elapsed: 0:24:26.\n",
            "  Batch 2,840  of  4,877.    Elapsed: 0:24:47.\n",
            "  Batch 2,880  of  4,877.    Elapsed: 0:25:08.\n",
            "  Batch 2,920  of  4,877.    Elapsed: 0:25:29.\n",
            "  Batch 2,960  of  4,877.    Elapsed: 0:25:50.\n",
            "  Batch 3,000  of  4,877.    Elapsed: 0:26:11.\n",
            "  Batch 3,040  of  4,877.    Elapsed: 0:26:32.\n",
            "  Batch 3,080  of  4,877.    Elapsed: 0:26:53.\n",
            "  Batch 3,120  of  4,877.    Elapsed: 0:27:14.\n",
            "  Batch 3,160  of  4,877.    Elapsed: 0:27:35.\n",
            "  Batch 3,200  of  4,877.    Elapsed: 0:27:56.\n",
            "  Batch 3,240  of  4,877.    Elapsed: 0:28:18.\n",
            "  Batch 3,280  of  4,877.    Elapsed: 0:28:39.\n",
            "  Batch 3,320  of  4,877.    Elapsed: 0:29:00.\n",
            "  Batch 3,360  of  4,877.    Elapsed: 0:29:21.\n",
            "  Batch 3,400  of  4,877.    Elapsed: 0:29:42.\n",
            "  Batch 3,440  of  4,877.    Elapsed: 0:30:03.\n",
            "  Batch 3,480  of  4,877.    Elapsed: 0:30:24.\n",
            "  Batch 3,520  of  4,877.    Elapsed: 0:30:45.\n",
            "  Batch 3,560  of  4,877.    Elapsed: 0:31:06.\n",
            "  Batch 3,600  of  4,877.    Elapsed: 0:31:27.\n",
            "  Batch 3,640  of  4,877.    Elapsed: 0:31:48.\n",
            "  Batch 3,680  of  4,877.    Elapsed: 0:32:09.\n",
            "  Batch 3,720  of  4,877.    Elapsed: 0:32:31.\n",
            "  Batch 3,760  of  4,877.    Elapsed: 0:32:52.\n",
            "  Batch 3,800  of  4,877.    Elapsed: 0:33:13.\n",
            "  Batch 3,840  of  4,877.    Elapsed: 0:33:34.\n",
            "  Batch 3,880  of  4,877.    Elapsed: 0:33:55.\n",
            "  Batch 3,920  of  4,877.    Elapsed: 0:34:16.\n",
            "  Batch 3,960  of  4,877.    Elapsed: 0:34:37.\n",
            "  Batch 4,000  of  4,877.    Elapsed: 0:34:58.\n",
            "  Batch 4,040  of  4,877.    Elapsed: 0:35:19.\n",
            "  Batch 4,080  of  4,877.    Elapsed: 0:35:40.\n",
            "  Batch 4,120  of  4,877.    Elapsed: 0:36:02.\n",
            "  Batch 4,160  of  4,877.    Elapsed: 0:36:23.\n",
            "  Batch 4,200  of  4,877.    Elapsed: 0:36:44.\n",
            "  Batch 4,240  of  4,877.    Elapsed: 0:37:05.\n",
            "  Batch 4,280  of  4,877.    Elapsed: 0:37:26.\n",
            "  Batch 4,320  of  4,877.    Elapsed: 0:37:47.\n",
            "  Batch 4,360  of  4,877.    Elapsed: 0:38:08.\n",
            "  Batch 4,400  of  4,877.    Elapsed: 0:38:29.\n",
            "  Batch 4,440  of  4,877.    Elapsed: 0:38:50.\n",
            "  Batch 4,480  of  4,877.    Elapsed: 0:39:11.\n",
            "  Batch 4,520  of  4,877.    Elapsed: 0:39:32.\n",
            "  Batch 4,560  of  4,877.    Elapsed: 0:39:53.\n",
            "  Batch 4,600  of  4,877.    Elapsed: 0:40:14.\n",
            "  Batch 4,640  of  4,877.    Elapsed: 0:40:35.\n",
            "  Batch 4,680  of  4,877.    Elapsed: 0:40:57.\n",
            "  Batch 4,720  of  4,877.    Elapsed: 0:41:18.\n",
            "  Batch 4,760  of  4,877.    Elapsed: 0:41:39.\n",
            "  Batch 4,800  of  4,877.    Elapsed: 0:42:00.\n",
            "  Batch 4,840  of  4,877.    Elapsed: 0:42:21.\n",
            "\n",
            "  Average training loss: 0.78\n",
            "  Training epcoh took: 0:42:40\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  4,877.    Elapsed: 0:00:21.\n",
            "  Batch    80  of  4,877.    Elapsed: 0:00:42.\n",
            "  Batch   120  of  4,877.    Elapsed: 0:01:03.\n",
            "  Batch   160  of  4,877.    Elapsed: 0:01:24.\n",
            "  Batch   200  of  4,877.    Elapsed: 0:01:45.\n",
            "  Batch   240  of  4,877.    Elapsed: 0:02:07.\n",
            "  Batch   280  of  4,877.    Elapsed: 0:02:28.\n",
            "  Batch   320  of  4,877.    Elapsed: 0:02:49.\n",
            "  Batch   360  of  4,877.    Elapsed: 0:03:10.\n",
            "  Batch   400  of  4,877.    Elapsed: 0:03:31.\n",
            "  Batch   440  of  4,877.    Elapsed: 0:03:52.\n",
            "  Batch   480  of  4,877.    Elapsed: 0:04:13.\n",
            "  Batch   520  of  4,877.    Elapsed: 0:04:34.\n",
            "  Batch   560  of  4,877.    Elapsed: 0:04:55.\n",
            "  Batch   600  of  4,877.    Elapsed: 0:05:16.\n",
            "  Batch   640  of  4,877.    Elapsed: 0:05:38.\n",
            "  Batch   680  of  4,877.    Elapsed: 0:05:59.\n",
            "  Batch   720  of  4,877.    Elapsed: 0:06:20.\n",
            "  Batch   760  of  4,877.    Elapsed: 0:06:41.\n",
            "  Batch   800  of  4,877.    Elapsed: 0:07:02.\n",
            "  Batch   840  of  4,877.    Elapsed: 0:07:23.\n",
            "  Batch   880  of  4,877.    Elapsed: 0:07:44.\n",
            "  Batch   920  of  4,877.    Elapsed: 0:08:05.\n",
            "  Batch   960  of  4,877.    Elapsed: 0:08:26.\n",
            "  Batch 1,000  of  4,877.    Elapsed: 0:08:47.\n",
            "  Batch 1,040  of  4,877.    Elapsed: 0:09:08.\n",
            "  Batch 1,080  of  4,877.    Elapsed: 0:09:29.\n",
            "  Batch 1,120  of  4,877.    Elapsed: 0:09:51.\n",
            "  Batch 1,160  of  4,877.    Elapsed: 0:10:12.\n",
            "  Batch 1,200  of  4,877.    Elapsed: 0:10:33.\n",
            "  Batch 1,240  of  4,877.    Elapsed: 0:10:54.\n",
            "  Batch 1,280  of  4,877.    Elapsed: 0:11:15.\n",
            "  Batch 1,320  of  4,877.    Elapsed: 0:11:36.\n",
            "  Batch 1,360  of  4,877.    Elapsed: 0:11:57.\n",
            "  Batch 1,400  of  4,877.    Elapsed: 0:12:18.\n",
            "  Batch 1,440  of  4,877.    Elapsed: 0:12:39.\n",
            "  Batch 1,480  of  4,877.    Elapsed: 0:13:00.\n",
            "  Batch 1,520  of  4,877.    Elapsed: 0:13:21.\n",
            "  Batch 1,560  of  4,877.    Elapsed: 0:13:43.\n",
            "  Batch 1,600  of  4,877.    Elapsed: 0:14:04.\n",
            "  Batch 1,640  of  4,877.    Elapsed: 0:14:25.\n",
            "  Batch 1,680  of  4,877.    Elapsed: 0:14:46.\n",
            "  Batch 1,720  of  4,877.    Elapsed: 0:15:07.\n",
            "  Batch 1,760  of  4,877.    Elapsed: 0:15:28.\n",
            "  Batch 1,800  of  4,877.    Elapsed: 0:15:49.\n",
            "  Batch 1,840  of  4,877.    Elapsed: 0:16:10.\n",
            "  Batch 1,880  of  4,877.    Elapsed: 0:16:31.\n",
            "  Batch 1,920  of  4,877.    Elapsed: 0:16:52.\n",
            "  Batch 1,960  of  4,877.    Elapsed: 0:17:13.\n",
            "  Batch 2,000  of  4,877.    Elapsed: 0:17:34.\n",
            "  Batch 2,040  of  4,877.    Elapsed: 0:17:55.\n",
            "  Batch 2,080  of  4,877.    Elapsed: 0:18:16.\n",
            "  Batch 2,120  of  4,877.    Elapsed: 0:18:37.\n",
            "  Batch 2,160  of  4,877.    Elapsed: 0:18:59.\n",
            "  Batch 2,200  of  4,877.    Elapsed: 0:19:20.\n",
            "  Batch 2,240  of  4,877.    Elapsed: 0:19:41.\n",
            "  Batch 2,280  of  4,877.    Elapsed: 0:20:02.\n",
            "  Batch 2,320  of  4,877.    Elapsed: 0:20:23.\n",
            "  Batch 2,360  of  4,877.    Elapsed: 0:20:44.\n",
            "  Batch 2,400  of  4,877.    Elapsed: 0:21:05.\n",
            "  Batch 2,440  of  4,877.    Elapsed: 0:21:26.\n",
            "  Batch 2,480  of  4,877.    Elapsed: 0:21:47.\n",
            "  Batch 2,520  of  4,877.    Elapsed: 0:22:08.\n",
            "  Batch 2,560  of  4,877.    Elapsed: 0:22:29.\n",
            "  Batch 2,600  of  4,877.    Elapsed: 0:22:50.\n",
            "  Batch 2,640  of  4,877.    Elapsed: 0:23:12.\n",
            "  Batch 2,680  of  4,877.    Elapsed: 0:23:33.\n",
            "  Batch 2,720  of  4,877.    Elapsed: 0:23:54.\n",
            "  Batch 2,760  of  4,877.    Elapsed: 0:24:15.\n",
            "  Batch 2,800  of  4,877.    Elapsed: 0:24:36.\n",
            "  Batch 2,840  of  4,877.    Elapsed: 0:24:57.\n",
            "  Batch 2,880  of  4,877.    Elapsed: 0:25:18.\n",
            "  Batch 2,920  of  4,877.    Elapsed: 0:25:39.\n",
            "  Batch 2,960  of  4,877.    Elapsed: 0:26:00.\n",
            "  Batch 3,000  of  4,877.    Elapsed: 0:26:21.\n",
            "  Batch 3,040  of  4,877.    Elapsed: 0:26:42.\n",
            "  Batch 3,080  of  4,877.    Elapsed: 0:27:03.\n",
            "  Batch 3,120  of  4,877.    Elapsed: 0:27:25.\n",
            "  Batch 3,160  of  4,877.    Elapsed: 0:27:46.\n",
            "  Batch 3,200  of  4,877.    Elapsed: 0:28:07.\n",
            "  Batch 3,240  of  4,877.    Elapsed: 0:28:28.\n",
            "  Batch 3,280  of  4,877.    Elapsed: 0:28:49.\n",
            "  Batch 3,320  of  4,877.    Elapsed: 0:29:10.\n",
            "  Batch 3,360  of  4,877.    Elapsed: 0:29:31.\n",
            "  Batch 3,400  of  4,877.    Elapsed: 0:29:52.\n",
            "  Batch 3,440  of  4,877.    Elapsed: 0:30:13.\n",
            "  Batch 3,480  of  4,877.    Elapsed: 0:30:34.\n",
            "  Batch 3,520  of  4,877.    Elapsed: 0:30:55.\n",
            "  Batch 3,560  of  4,877.    Elapsed: 0:31:16.\n",
            "  Batch 3,600  of  4,877.    Elapsed: 0:31:38.\n",
            "  Batch 3,640  of  4,877.    Elapsed: 0:31:59.\n",
            "  Batch 3,680  of  4,877.    Elapsed: 0:32:20.\n",
            "  Batch 3,720  of  4,877.    Elapsed: 0:32:41.\n",
            "  Batch 3,760  of  4,877.    Elapsed: 0:33:02.\n",
            "  Batch 3,800  of  4,877.    Elapsed: 0:33:23.\n",
            "  Batch 3,840  of  4,877.    Elapsed: 0:33:44.\n",
            "  Batch 3,880  of  4,877.    Elapsed: 0:34:05.\n",
            "  Batch 3,920  of  4,877.    Elapsed: 0:34:26.\n",
            "  Batch 3,960  of  4,877.    Elapsed: 0:34:47.\n",
            "  Batch 4,000  of  4,877.    Elapsed: 0:35:08.\n",
            "  Batch 4,040  of  4,877.    Elapsed: 0:35:29.\n",
            "  Batch 4,080  of  4,877.    Elapsed: 0:35:50.\n",
            "  Batch 4,120  of  4,877.    Elapsed: 0:36:11.\n",
            "  Batch 4,160  of  4,877.    Elapsed: 0:36:33.\n",
            "  Batch 4,200  of  4,877.    Elapsed: 0:36:54.\n",
            "  Batch 4,240  of  4,877.    Elapsed: 0:37:15.\n",
            "  Batch 4,280  of  4,877.    Elapsed: 0:37:36.\n",
            "  Batch 4,320  of  4,877.    Elapsed: 0:37:57.\n",
            "  Batch 4,360  of  4,877.    Elapsed: 0:38:18.\n",
            "  Batch 4,400  of  4,877.    Elapsed: 0:38:39.\n",
            "  Batch 4,440  of  4,877.    Elapsed: 0:39:00.\n",
            "  Batch 4,480  of  4,877.    Elapsed: 0:39:21.\n",
            "  Batch 4,520  of  4,877.    Elapsed: 0:39:42.\n",
            "  Batch 4,560  of  4,877.    Elapsed: 0:40:03.\n",
            "  Batch 4,600  of  4,877.    Elapsed: 0:40:24.\n",
            "  Batch 4,640  of  4,877.    Elapsed: 0:40:45.\n",
            "  Batch 4,680  of  4,877.    Elapsed: 0:41:06.\n",
            "  Batch 4,720  of  4,877.    Elapsed: 0:41:27.\n",
            "  Batch 4,760  of  4,877.    Elapsed: 0:41:49.\n",
            "  Batch 4,800  of  4,877.    Elapsed: 0:42:10.\n",
            "  Batch 4,840  of  4,877.    Elapsed: 0:42:31.\n",
            "\n",
            "  Average training loss: 0.65\n",
            "  Training epcoh took: 0:42:50\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  4,877.    Elapsed: 0:00:21.\n",
            "  Batch    80  of  4,877.    Elapsed: 0:00:42.\n",
            "  Batch   120  of  4,877.    Elapsed: 0:01:03.\n",
            "  Batch   160  of  4,877.    Elapsed: 0:01:24.\n",
            "  Batch   200  of  4,877.    Elapsed: 0:01:45.\n",
            "  Batch   240  of  4,877.    Elapsed: 0:02:07.\n",
            "  Batch   280  of  4,877.    Elapsed: 0:02:28.\n",
            "  Batch   320  of  4,877.    Elapsed: 0:02:49.\n",
            "  Batch   360  of  4,877.    Elapsed: 0:03:10.\n",
            "  Batch   400  of  4,877.    Elapsed: 0:03:31.\n",
            "  Batch   440  of  4,877.    Elapsed: 0:03:52.\n",
            "  Batch   480  of  4,877.    Elapsed: 0:04:13.\n",
            "  Batch   520  of  4,877.    Elapsed: 0:04:34.\n",
            "  Batch   560  of  4,877.    Elapsed: 0:04:55.\n",
            "  Batch   600  of  4,877.    Elapsed: 0:05:16.\n",
            "  Batch   640  of  4,877.    Elapsed: 0:05:37.\n",
            "  Batch   680  of  4,877.    Elapsed: 0:05:58.\n",
            "  Batch   720  of  4,877.    Elapsed: 0:06:19.\n",
            "  Batch   760  of  4,877.    Elapsed: 0:06:40.\n",
            "  Batch   800  of  4,877.    Elapsed: 0:07:02.\n",
            "  Batch   840  of  4,877.    Elapsed: 0:07:23.\n",
            "  Batch   880  of  4,877.    Elapsed: 0:07:44.\n",
            "  Batch   920  of  4,877.    Elapsed: 0:08:05.\n",
            "  Batch   960  of  4,877.    Elapsed: 0:08:26.\n",
            "  Batch 1,000  of  4,877.    Elapsed: 0:08:47.\n",
            "  Batch 1,040  of  4,877.    Elapsed: 0:09:08.\n",
            "  Batch 1,080  of  4,877.    Elapsed: 0:09:29.\n",
            "  Batch 1,120  of  4,877.    Elapsed: 0:09:50.\n",
            "  Batch 1,160  of  4,877.    Elapsed: 0:10:11.\n",
            "  Batch 1,200  of  4,877.    Elapsed: 0:10:32.\n",
            "  Batch 1,240  of  4,877.    Elapsed: 0:10:53.\n",
            "  Batch 1,280  of  4,877.    Elapsed: 0:11:15.\n",
            "  Batch 1,320  of  4,877.    Elapsed: 0:11:36.\n",
            "  Batch 1,360  of  4,877.    Elapsed: 0:11:57.\n",
            "  Batch 1,400  of  4,877.    Elapsed: 0:12:18.\n",
            "  Batch 1,440  of  4,877.    Elapsed: 0:12:39.\n",
            "  Batch 1,480  of  4,877.    Elapsed: 0:13:00.\n",
            "  Batch 1,520  of  4,877.    Elapsed: 0:13:21.\n",
            "  Batch 1,560  of  4,877.    Elapsed: 0:13:42.\n",
            "  Batch 1,600  of  4,877.    Elapsed: 0:14:03.\n",
            "  Batch 1,640  of  4,877.    Elapsed: 0:14:24.\n",
            "  Batch 1,680  of  4,877.    Elapsed: 0:14:45.\n",
            "  Batch 1,720  of  4,877.    Elapsed: 0:15:07.\n",
            "  Batch 1,760  of  4,877.    Elapsed: 0:15:28.\n",
            "  Batch 1,800  of  4,877.    Elapsed: 0:15:49.\n",
            "  Batch 1,840  of  4,877.    Elapsed: 0:16:10.\n",
            "  Batch 1,880  of  4,877.    Elapsed: 0:16:31.\n",
            "  Batch 1,920  of  4,877.    Elapsed: 0:16:52.\n",
            "  Batch 1,960  of  4,877.    Elapsed: 0:17:13.\n",
            "  Batch 2,000  of  4,877.    Elapsed: 0:17:34.\n",
            "  Batch 2,040  of  4,877.    Elapsed: 0:17:55.\n",
            "  Batch 2,080  of  4,877.    Elapsed: 0:18:17.\n",
            "  Batch 2,120  of  4,877.    Elapsed: 0:18:38.\n",
            "  Batch 2,160  of  4,877.    Elapsed: 0:18:59.\n",
            "  Batch 2,200  of  4,877.    Elapsed: 0:19:20.\n",
            "  Batch 2,240  of  4,877.    Elapsed: 0:19:41.\n",
            "  Batch 2,280  of  4,877.    Elapsed: 0:20:02.\n",
            "  Batch 2,320  of  4,877.    Elapsed: 0:20:23.\n",
            "  Batch 2,360  of  4,877.    Elapsed: 0:20:44.\n",
            "  Batch 2,400  of  4,877.    Elapsed: 0:21:05.\n",
            "  Batch 2,440  of  4,877.    Elapsed: 0:21:27.\n",
            "  Batch 2,480  of  4,877.    Elapsed: 0:21:48.\n",
            "  Batch 2,520  of  4,877.    Elapsed: 0:22:09.\n",
            "  Batch 2,560  of  4,877.    Elapsed: 0:22:30.\n",
            "  Batch 2,600  of  4,877.    Elapsed: 0:22:51.\n",
            "  Batch 2,640  of  4,877.    Elapsed: 0:23:12.\n",
            "  Batch 2,680  of  4,877.    Elapsed: 0:23:33.\n",
            "  Batch 2,720  of  4,877.    Elapsed: 0:23:54.\n",
            "  Batch 2,760  of  4,877.    Elapsed: 0:24:15.\n",
            "  Batch 2,800  of  4,877.    Elapsed: 0:24:37.\n",
            "  Batch 2,840  of  4,877.    Elapsed: 0:24:58.\n",
            "  Batch 2,880  of  4,877.    Elapsed: 0:25:19.\n",
            "  Batch 2,920  of  4,877.    Elapsed: 0:25:40.\n",
            "  Batch 2,960  of  4,877.    Elapsed: 0:26:01.\n",
            "  Batch 3,000  of  4,877.    Elapsed: 0:26:22.\n",
            "  Batch 3,040  of  4,877.    Elapsed: 0:26:43.\n",
            "  Batch 3,080  of  4,877.    Elapsed: 0:27:04.\n",
            "  Batch 3,120  of  4,877.    Elapsed: 0:27:25.\n",
            "  Batch 3,160  of  4,877.    Elapsed: 0:27:46.\n",
            "  Batch 3,200  of  4,877.    Elapsed: 0:28:07.\n",
            "  Batch 3,240  of  4,877.    Elapsed: 0:28:29.\n",
            "  Batch 3,280  of  4,877.    Elapsed: 0:28:50.\n",
            "  Batch 3,320  of  4,877.    Elapsed: 0:29:11.\n",
            "  Batch 3,360  of  4,877.    Elapsed: 0:29:32.\n",
            "  Batch 3,400  of  4,877.    Elapsed: 0:29:53.\n",
            "  Batch 3,440  of  4,877.    Elapsed: 0:30:14.\n",
            "  Batch 3,480  of  4,877.    Elapsed: 0:30:35.\n",
            "  Batch 3,520  of  4,877.    Elapsed: 0:30:56.\n",
            "  Batch 3,560  of  4,877.    Elapsed: 0:31:17.\n",
            "  Batch 3,600  of  4,877.    Elapsed: 0:31:38.\n",
            "  Batch 3,640  of  4,877.    Elapsed: 0:31:59.\n",
            "  Batch 3,680  of  4,877.    Elapsed: 0:32:21.\n",
            "  Batch 3,720  of  4,877.    Elapsed: 0:32:42.\n",
            "  Batch 3,760  of  4,877.    Elapsed: 0:33:03.\n",
            "  Batch 3,800  of  4,877.    Elapsed: 0:33:24.\n",
            "  Batch 3,840  of  4,877.    Elapsed: 0:33:45.\n",
            "  Batch 3,880  of  4,877.    Elapsed: 0:34:06.\n",
            "  Batch 3,920  of  4,877.    Elapsed: 0:34:27.\n",
            "  Batch 3,960  of  4,877.    Elapsed: 0:34:48.\n",
            "  Batch 4,000  of  4,877.    Elapsed: 0:35:09.\n",
            "  Batch 4,040  of  4,877.    Elapsed: 0:35:30.\n",
            "  Batch 4,080  of  4,877.    Elapsed: 0:35:51.\n",
            "  Batch 4,120  of  4,877.    Elapsed: 0:36:12.\n",
            "  Batch 4,160  of  4,877.    Elapsed: 0:36:33.\n",
            "  Batch 4,200  of  4,877.    Elapsed: 0:36:54.\n",
            "  Batch 4,240  of  4,877.    Elapsed: 0:37:16.\n",
            "  Batch 4,280  of  4,877.    Elapsed: 0:37:37.\n",
            "  Batch 4,320  of  4,877.    Elapsed: 0:37:58.\n",
            "  Batch 4,360  of  4,877.    Elapsed: 0:38:19.\n",
            "  Batch 4,400  of  4,877.    Elapsed: 0:38:40.\n",
            "  Batch 4,440  of  4,877.    Elapsed: 0:39:01.\n",
            "  Batch 4,480  of  4,877.    Elapsed: 0:39:22.\n",
            "  Batch 4,520  of  4,877.    Elapsed: 0:39:43.\n",
            "  Batch 4,560  of  4,877.    Elapsed: 0:40:04.\n",
            "  Batch 4,600  of  4,877.    Elapsed: 0:40:26.\n",
            "  Batch 4,640  of  4,877.    Elapsed: 0:40:47.\n",
            "  Batch 4,680  of  4,877.    Elapsed: 0:41:08.\n",
            "  Batch 4,720  of  4,877.    Elapsed: 0:41:29.\n",
            "  Batch 4,760  of  4,877.    Elapsed: 0:41:50.\n",
            "  Batch 4,800  of  4,877.    Elapsed: 0:42:11.\n",
            "  Batch 4,840  of  4,877.    Elapsed: 0:42:32.\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epcoh took: 0:42:51\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  4,877.    Elapsed: 0:00:21.\n",
            "  Batch    80  of  4,877.    Elapsed: 0:00:42.\n",
            "  Batch   120  of  4,877.    Elapsed: 0:01:03.\n",
            "  Batch   160  of  4,877.    Elapsed: 0:01:24.\n",
            "  Batch   200  of  4,877.    Elapsed: 0:01:46.\n",
            "  Batch   240  of  4,877.    Elapsed: 0:02:07.\n",
            "  Batch   280  of  4,877.    Elapsed: 0:02:28.\n",
            "  Batch   320  of  4,877.    Elapsed: 0:02:49.\n",
            "  Batch   360  of  4,877.    Elapsed: 0:03:10.\n",
            "  Batch   400  of  4,877.    Elapsed: 0:03:31.\n",
            "  Batch   440  of  4,877.    Elapsed: 0:03:52.\n",
            "  Batch   480  of  4,877.    Elapsed: 0:04:13.\n",
            "  Batch   520  of  4,877.    Elapsed: 0:04:34.\n",
            "  Batch   560  of  4,877.    Elapsed: 0:04:55.\n",
            "  Batch   600  of  4,877.    Elapsed: 0:05:16.\n",
            "  Batch   640  of  4,877.    Elapsed: 0:05:38.\n",
            "  Batch   680  of  4,877.    Elapsed: 0:05:59.\n",
            "  Batch   720  of  4,877.    Elapsed: 0:06:20.\n",
            "  Batch   760  of  4,877.    Elapsed: 0:06:41.\n",
            "  Batch   800  of  4,877.    Elapsed: 0:07:02.\n",
            "  Batch   840  of  4,877.    Elapsed: 0:07:23.\n",
            "  Batch   880  of  4,877.    Elapsed: 0:07:44.\n",
            "  Batch   920  of  4,877.    Elapsed: 0:08:05.\n",
            "  Batch   960  of  4,877.    Elapsed: 0:08:26.\n",
            "  Batch 1,000  of  4,877.    Elapsed: 0:08:47.\n",
            "  Batch 1,040  of  4,877.    Elapsed: 0:09:08.\n",
            "  Batch 1,080  of  4,877.    Elapsed: 0:09:30.\n",
            "  Batch 1,120  of  4,877.    Elapsed: 0:09:51.\n",
            "  Batch 1,160  of  4,877.    Elapsed: 0:10:12.\n",
            "  Batch 1,200  of  4,877.    Elapsed: 0:10:33.\n",
            "  Batch 1,240  of  4,877.    Elapsed: 0:10:54.\n",
            "  Batch 1,280  of  4,877.    Elapsed: 0:11:15.\n",
            "  Batch 1,320  of  4,877.    Elapsed: 0:11:36.\n",
            "  Batch 1,360  of  4,877.    Elapsed: 0:11:57.\n",
            "  Batch 1,400  of  4,877.    Elapsed: 0:12:18.\n",
            "  Batch 1,440  of  4,877.    Elapsed: 0:12:39.\n",
            "  Batch 1,480  of  4,877.    Elapsed: 0:13:01.\n",
            "  Batch 1,520  of  4,877.    Elapsed: 0:13:22.\n",
            "  Batch 1,560  of  4,877.    Elapsed: 0:13:43.\n",
            "  Batch 1,600  of  4,877.    Elapsed: 0:14:04.\n",
            "  Batch 1,640  of  4,877.    Elapsed: 0:14:25.\n",
            "  Batch 1,680  of  4,877.    Elapsed: 0:14:46.\n",
            "  Batch 1,720  of  4,877.    Elapsed: 0:15:07.\n",
            "  Batch 1,760  of  4,877.    Elapsed: 0:15:28.\n",
            "  Batch 1,800  of  4,877.    Elapsed: 0:15:49.\n",
            "  Batch 1,840  of  4,877.    Elapsed: 0:16:10.\n",
            "  Batch 1,880  of  4,877.    Elapsed: 0:16:31.\n",
            "  Batch 1,920  of  4,877.    Elapsed: 0:16:53.\n",
            "  Batch 1,960  of  4,877.    Elapsed: 0:17:14.\n",
            "  Batch 2,000  of  4,877.    Elapsed: 0:17:35.\n",
            "  Batch 2,040  of  4,877.    Elapsed: 0:17:56.\n",
            "  Batch 2,080  of  4,877.    Elapsed: 0:18:17.\n",
            "  Batch 2,120  of  4,877.    Elapsed: 0:18:38.\n",
            "  Batch 2,160  of  4,877.    Elapsed: 0:18:59.\n",
            "  Batch 2,200  of  4,877.    Elapsed: 0:19:20.\n",
            "  Batch 2,240  of  4,877.    Elapsed: 0:19:41.\n",
            "  Batch 2,280  of  4,877.    Elapsed: 0:20:03.\n",
            "  Batch 2,320  of  4,877.    Elapsed: 0:20:24.\n",
            "  Batch 2,360  of  4,877.    Elapsed: 0:20:45.\n",
            "  Batch 2,400  of  4,877.    Elapsed: 0:21:06.\n",
            "  Batch 2,440  of  4,877.    Elapsed: 0:21:27.\n",
            "  Batch 2,480  of  4,877.    Elapsed: 0:21:48.\n",
            "  Batch 2,520  of  4,877.    Elapsed: 0:22:09.\n",
            "  Batch 2,560  of  4,877.    Elapsed: 0:22:30.\n",
            "  Batch 2,600  of  4,877.    Elapsed: 0:22:51.\n",
            "  Batch 2,640  of  4,877.    Elapsed: 0:23:12.\n",
            "  Batch 2,680  of  4,877.    Elapsed: 0:23:33.\n",
            "  Batch 2,720  of  4,877.    Elapsed: 0:23:54.\n",
            "  Batch 2,760  of  4,877.    Elapsed: 0:24:15.\n",
            "  Batch 2,800  of  4,877.    Elapsed: 0:24:36.\n",
            "  Batch 2,840  of  4,877.    Elapsed: 0:24:58.\n",
            "  Batch 2,880  of  4,877.    Elapsed: 0:25:19.\n",
            "  Batch 2,920  of  4,877.    Elapsed: 0:25:40.\n",
            "  Batch 2,960  of  4,877.    Elapsed: 0:26:01.\n",
            "  Batch 3,000  of  4,877.    Elapsed: 0:26:22.\n",
            "  Batch 3,040  of  4,877.    Elapsed: 0:26:43.\n",
            "  Batch 3,080  of  4,877.    Elapsed: 0:27:04.\n",
            "  Batch 3,120  of  4,877.    Elapsed: 0:27:25.\n",
            "  Batch 3,160  of  4,877.    Elapsed: 0:27:46.\n",
            "  Batch 3,200  of  4,877.    Elapsed: 0:28:07.\n",
            "  Batch 3,240  of  4,877.    Elapsed: 0:28:28.\n",
            "  Batch 3,280  of  4,877.    Elapsed: 0:28:50.\n",
            "  Batch 3,320  of  4,877.    Elapsed: 0:29:11.\n",
            "  Batch 3,360  of  4,877.    Elapsed: 0:29:32.\n",
            "  Batch 3,400  of  4,877.    Elapsed: 0:29:53.\n",
            "  Batch 3,440  of  4,877.    Elapsed: 0:30:14.\n",
            "  Batch 3,480  of  4,877.    Elapsed: 0:30:35.\n",
            "  Batch 3,520  of  4,877.    Elapsed: 0:30:56.\n",
            "  Batch 3,560  of  4,877.    Elapsed: 0:31:17.\n",
            "  Batch 3,600  of  4,877.    Elapsed: 0:31:38.\n",
            "  Batch 3,640  of  4,877.    Elapsed: 0:32:00.\n",
            "  Batch 3,680  of  4,877.    Elapsed: 0:32:21.\n",
            "  Batch 3,720  of  4,877.    Elapsed: 0:32:42.\n",
            "  Batch 3,760  of  4,877.    Elapsed: 0:33:03.\n",
            "  Batch 3,800  of  4,877.    Elapsed: 0:33:24.\n",
            "  Batch 3,840  of  4,877.    Elapsed: 0:33:45.\n",
            "  Batch 3,880  of  4,877.    Elapsed: 0:34:06.\n",
            "  Batch 3,920  of  4,877.    Elapsed: 0:34:27.\n",
            "  Batch 3,960  of  4,877.    Elapsed: 0:34:48.\n",
            "  Batch 4,000  of  4,877.    Elapsed: 0:35:09.\n",
            "  Batch 4,040  of  4,877.    Elapsed: 0:35:30.\n",
            "  Batch 4,080  of  4,877.    Elapsed: 0:35:51.\n",
            "  Batch 4,120  of  4,877.    Elapsed: 0:36:13.\n",
            "  Batch 4,160  of  4,877.    Elapsed: 0:36:34.\n",
            "  Batch 4,200  of  4,877.    Elapsed: 0:36:55.\n",
            "  Batch 4,240  of  4,877.    Elapsed: 0:37:16.\n",
            "  Batch 4,280  of  4,877.    Elapsed: 0:37:37.\n",
            "  Batch 4,320  of  4,877.    Elapsed: 0:37:58.\n",
            "  Batch 4,360  of  4,877.    Elapsed: 0:38:19.\n",
            "  Batch 4,400  of  4,877.    Elapsed: 0:38:41.\n",
            "  Batch 4,440  of  4,877.    Elapsed: 0:39:02.\n",
            "  Batch 4,480  of  4,877.    Elapsed: 0:39:23.\n",
            "  Batch 4,520  of  4,877.    Elapsed: 0:39:44.\n",
            "  Batch 4,560  of  4,877.    Elapsed: 0:40:05.\n",
            "  Batch 4,600  of  4,877.    Elapsed: 0:40:26.\n",
            "  Batch 4,640  of  4,877.    Elapsed: 0:40:47.\n",
            "  Batch 4,680  of  4,877.    Elapsed: 0:41:08.\n",
            "  Batch 4,720  of  4,877.    Elapsed: 0:41:29.\n",
            "  Batch 4,760  of  4,877.    Elapsed: 0:41:50.\n",
            "  Batch 4,800  of  4,877.    Elapsed: 0:42:11.\n",
            "  Batch 4,840  of  4,877.    Elapsed: 0:42:32.\n",
            "\n",
            "  Average training loss: 0.51\n",
            "  Training epcoh took: 0:42:52\n",
            "\n",
            "Training complete!\n",
            "Total training took 2:51:14 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8vT9lN_VOjk",
        "colab_type": "code",
        "outputId": "bdb44284-a8d2-4f5c-85e7-34617cbcf0b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Training Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.78</td>\n",
              "      <td>0:42:40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.65</td>\n",
              "      <td>0:42:50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.57</td>\n",
              "      <td>0:42:51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.51</td>\n",
              "      <td>0:42:52</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss Training Time\n",
              "epoch                             \n",
              "1               0.78       0:42:40\n",
              "2               0.65       0:42:50\n",
              "3               0.57       0:42:51\n",
              "4               0.51       0:42:52"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFOYhwOLVWmm",
        "colab_type": "code",
        "outputId": "bfe1a5d9-05a4-4e69-e626-37941a797c4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "#We do not have validation labels t\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "# plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60uJ0z5X_ybO",
        "colab_type": "code",
        "outputId": "de677e09-6bfb-4636-9aae-2e46abfa9019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(val_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions = []\n",
        "\n",
        "# Predict \n",
        "for batch in val_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  # label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  # true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 66,292 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCIW87IFZKp8",
        "colab_type": "code",
        "outputId": "09864135-fcbc-4fe8-c793-e9e1e1e76613",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import itertools\n",
        "y_pred = []\n",
        "for i in range(len(predictions)):\n",
        "  \n",
        "  # The predictions for this batch are a 5-column ndarray with a row for each batch (32). \n",
        "  #Pick the label with the highest value and consider the index = class\n",
        "  pred_labels_i = list(np.argmax(predictions[i], axis=1))\n",
        "  y_pred.append(pred_labels_i)\n",
        "\n",
        "y_pred = list(itertools.chain.from_iterable(y_pred))\n",
        "y_pred[0:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 2, 2, 2, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsT-2TG5eDzs",
        "colab_type": "code",
        "outputId": "60461829-b18f-4779-daa5-80c037594938",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLCurel7fgLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final = pd.DataFrame()\n",
        "final['PhraseId'] = X_test['PhraseId']\n",
        "final['Sentiment'] = y_pred\n",
        "\n",
        "final.to_csv('/drive/My Drive/submission.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
